{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational autoencoders for collaborative filtering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook accompanies the paper \"*Variational autoencoders for collaborative filtering*\" by Dawen Liang, Rahul G. Krishnan, Matthew D. Hoffman, and Tony Jebara, in The Web Conference (aka WWW) 2018.\n",
    "\n",
    "In this notebook, we will show a complete self-contained example of training a variational autoencoder (as well as a denoising autoencoder) with multinomial likelihood (described in the paper) on the public Movielens-20M dataset, including both data preprocessing and model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sn\n",
    "sn.set()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import apply_regularization, l2_regularizer\n",
    "\n",
    "import bottleneck as bn\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data and create train/validation/test splits following strong generalization: \n",
    "\n",
    "- We split all users into training/validation/test sets. \n",
    "\n",
    "- We train models using the entire click history of the training users. \n",
    "\n",
    "- To evaluate, we take part of the click history from held-out (validation and test) users to learn the necessary user-level representations for the model and then compute metrics by looking at how well the model ranks the rest of the unseen click history from the held-out users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, download the dataset at http://files.grouplens.org/datasets/movielens/ml-20m.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### change `DATA_DIR` to the location where movielens-20m dataset sits\n",
    "DATA_DIR =\"C:/Users/atazarv/Code/Webscope_R1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(os.path.join(DATA_DIR, 'ratings.txt'),sep='\\t', names = [\"userId\", \"movieId\", \"rating\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115579440, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = raw_data[raw_data['rating']>95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = raw_data[raw_data['rating']<101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14569437, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userId           1\n",
       "movieId    1006373\n",
       "rating         100\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1006373</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1007035</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1007098</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1007723</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1008659</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating\n",
       "1       1  1006373     100\n",
       "3       1  1007035     100\n",
       "4       1  1007098     100\n",
       "5       1  1007723     100\n",
       "6       1  1008659     100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splitting procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Select 10K users as heldout users, 10K users as validation users, and the rest of the users for training\n",
    "- Use all the items from the training users as item set\n",
    "- For each of both validation and test user, subsample 80% as fold-in data and the rest for prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(tp, id):\n",
    "    playcount_groupbyid = tp[[id]].groupby(id, as_index=False)\n",
    "    count = playcount_groupbyid.size()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_triplets(tp, min_uc=5, min_sc=0):\n",
    "    # Only keep the triplets for items which were clicked on by at least min_sc users. \n",
    "    if min_sc > 0:\n",
    "        itemcount = get_count(tp, 'movieId')\n",
    "        tp = tp[tp['movieId'].isin(itemcount.index[itemcount >= min_sc])]\n",
    "    \n",
    "    # Only keep the triplets for users who clicked on at least min_uc items\n",
    "    # After doing this, some of the items will have less than min_uc users, but should only be a small proportion\n",
    "    if min_uc > 0:\n",
    "        usercount = get_count(tp, 'userId')\n",
    "        tp = tp[tp['userId'].isin(usercount.index[usercount >= min_uc])]\n",
    "    \n",
    "    # Update both usercount and itemcount after filtering\n",
    "    usercount, itemcount = get_count(tp, 'userId'), get_count(tp, 'movieId') \n",
    "    return tp, usercount, itemcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_activity, item_popularity = get_count(raw_data, 'userId'), get_count(raw_data, 'movieId') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtering, there are 14569437 watching events from 806664 users and 37704 movies (sparsity: 0.048%)\n"
     ]
    }
   ],
   "source": [
    "sparsity = 1. * raw_data.shape[0] / (user_activity.shape[0] * item_popularity.shape[0])\n",
    "\n",
    "print(\"Before filtering, there are %d watching events from %d users and %d movies (sparsity: %.3f%%)\" % \n",
    "      (raw_data.shape[0], user_activity.shape[0], item_popularity.shape[0], sparsity * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only keep items that are clicked on by at least 5 users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "raw_data, user_activity, item_popularity = filter_triplets(raw_data, min_uc=40, min_sc=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering, there are 7816645 watching events from 94375 users and 9950 movies (sparsity: 0.832%)\n"
     ]
    }
   ],
   "source": [
    "sparsity = 1. * raw_data.shape[0] / (user_activity.shape[0] * item_popularity.shape[0])\n",
    "\n",
    "print(\"After filtering, there are %d watching events from %d users and %d movies (sparsity: %.3f%%)\" % \n",
    "      (raw_data.shape[0], user_activity.shape[0], item_popularity.shape[0], sparsity * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_uid = user_activity.index\n",
    "\n",
    "np.random.seed(98765)\n",
    "idx_perm = np.random.permutation(unique_uid.size)\n",
    "unique_uid = unique_uid[idx_perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94375"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_uid.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train/validation/test users\n",
    "n_users = unique_uid.size\n",
    "n_heldout_users = 10000\n",
    "\n",
    "tr_users = unique_uid[:(n_users - n_heldout_users * 2)]\n",
    "vd_users = unique_uid[(n_users - n_heldout_users * 2): (n_users - n_heldout_users)]\n",
    "te_users = unique_uid[(n_users - n_heldout_users):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_plays = raw_data.loc[raw_data['userId'].isin(tr_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sid = pd.unique(train_plays['movieId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "show2id = dict((sid, i) for (i, sid) in enumerate(unique_sid))\n",
    "profile2id = dict((pid, i) for (i, pid) in enumerate(unique_uid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_dir = os.path.join(DATA_DIR, 'pro_sg')\n",
    "\n",
    "if not os.path.exists(pro_dir):\n",
    "    os.makedirs(pro_dir)\n",
    "\n",
    "with open(os.path.join(pro_dir, 'unique_sid.txt'), 'w') as f:\n",
    "    for sid in unique_sid:\n",
    "        f.write('%s\\n' % sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_proportion(data, test_prop=0.2):\n",
    "    data_grouped_by_user = data.groupby('userId')\n",
    "    tr_list, te_list = list(), list()\n",
    "\n",
    "    np.random.seed(98765)\n",
    "\n",
    "    for i, (_, group) in enumerate(data_grouped_by_user):\n",
    "        n_items_u = len(group)\n",
    "\n",
    "        if n_items_u >= 5:\n",
    "            idx = np.zeros(n_items_u, dtype='bool')\n",
    "            idx[np.random.choice(n_items_u, size=int(test_prop * n_items_u), replace=False).astype('int64')] = True\n",
    "\n",
    "            tr_list.append(group[np.logical_not(idx)])\n",
    "            te_list.append(group[idx])\n",
    "        else:\n",
    "            tr_list.append(group)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(\"%d users sampled\" % i)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    data_tr = pd.concat(tr_list)\n",
    "    data_te = pd.concat(te_list)\n",
    "    \n",
    "    return data_tr, data_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vad_plays = raw_data.loc[raw_data['userId'].isin(vd_users)]\n",
    "vad_plays = vad_plays.loc[vad_plays['movieId'].isin(unique_sid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 users sampled\n",
      "1000 users sampled\n",
      "2000 users sampled\n",
      "3000 users sampled\n",
      "4000 users sampled\n",
      "5000 users sampled\n",
      "6000 users sampled\n",
      "7000 users sampled\n",
      "8000 users sampled\n",
      "9000 users sampled\n"
     ]
    }
   ],
   "source": [
    "vad_plays_tr, vad_plays_te= split_train_test_proportion(vad_plays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(823076, 3)\n",
      "(823076, 3)\n"
     ]
    }
   ],
   "source": [
    "test_plays = raw_data.loc[raw_data['userId'].isin(te_users)]\n",
    "print(test_plays.shape)\n",
    "test_plays = test_plays.loc[test_plays['movieId'].isin(unique_sid)]\n",
    "print(test_plays.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 users sampled\n",
      "1000 users sampled\n",
      "2000 users sampled\n",
      "3000 users sampled\n",
      "4000 users sampled\n",
      "5000 users sampled\n",
      "6000 users sampled\n",
      "7000 users sampled\n",
      "8000 users sampled\n",
      "9000 users sampled\n"
     ]
    }
   ],
   "source": [
    "test_plays_tr, test_plays_te = split_train_test_proportion(test_plays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the data into (user_index, item_index) format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerize(tp):\n",
    "    uid = list(map(lambda x: profile2id[x], tp['userId']))\n",
    "    sid = list(map(lambda x: show2id[x], tp['movieId']))\n",
    "    return pd.DataFrame(data={'uid': uid, 'sid': sid}, columns=['uid', 'sid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = numerize(train_plays)\n",
    "train_data.to_csv(os.path.join(pro_dir, 'train.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vad_data_tr = numerize(vad_plays_tr)\n",
    "vad_data_tr.to_csv(os.path.join(pro_dir, 'validation_tr.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vad_data_te = numerize(vad_plays_te)\n",
    "vad_data_te.to_csv(os.path.join(pro_dir, 'validation_te.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_tr = numerize(test_plays_tr)\n",
    "test_data_tr.to_csv(os.path.join(pro_dir, 'test_tr.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_te = numerize(test_plays_te)\n",
    "test_data_te.to_csv(os.path.join(pro_dir, 'test_te.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define two related models: denoising autoencoder with multinomial likelihood (Multi-DAE in the paper) and partially-regularized variational autoencoder with multinomial likelihood (Multi-VAE^{PR} in the paper)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Notations__: We use $u \\in \\{1,\\dots,U\\}$ to index users and $i \\in \\{1,\\dots,I\\}$ to index items. In this work, we consider learning with implicit feedback. The user-by-item interaction matrix is the click matrix $\\mathbf{X} \\in \\mathbb{N}^{U\\times I}$. The lower case $\\mathbf{x}_u =[X_{u1},\\dots,X_{uI}]^\\top \\in \\mathbb{N}^I$ is a bag-of-words vector with the number of clicks for each item from user u. We binarize the click matrix. It is straightforward to extend it to general count data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Generative process__: For each user $u$, the model starts by sampling a $K$-dimensional latent representation $\\mathbf{z}_u$ from a standard Gaussian prior. The latent representation $\\mathbf{z}_u$ is transformed via a non-linear function $f_\\theta (\\cdot) \\in \\mathbb{R}^I$ to produce a probability distribution over $I$ items $\\pi (\\mathbf{z}_u)$ from which the click history $\\mathbf{x}_u$ is assumed to have been drawn:\n",
    "\n",
    "$$\n",
    "\\mathbf{z}_u \\sim \\mathcal{N}(0, \\mathbf{I}_K),  \\pi(\\mathbf{z}_u) \\propto \\exp\\{f_\\theta (\\mathbf{z}_u\\},\\\\\n",
    "\\mathbf{x}_u \\sim \\mathrm{Mult}(N_u, \\pi(\\mathbf{z}_u))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective for Multi-DAE for a single user $u$ is:\n",
    "$$\n",
    "\\mathcal{L}_u(\\theta, \\phi) = \\log p_\\theta(\\mathbf{x}_u | g_\\phi(\\mathbf{x}_u))\n",
    "$$\n",
    "where $g_\\phi(\\cdot)$ is the non-linear \"encoder\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiDAE(object):\n",
    "    def __init__(self, p_dims, q_dims=None, lam=0.01, lr=1e-3, random_seed=None):\n",
    "        self.p_dims = p_dims\n",
    "        if q_dims is None:\n",
    "            self.q_dims = p_dims[::-1]\n",
    "        else:\n",
    "            assert q_dims[0] == p_dims[-1], \"Input and output dimension must equal each other for autoencoders.\"\n",
    "            assert q_dims[-1] == p_dims[0], \"Latent dimension for p- and q-network mismatches.\"\n",
    "            self.q_dims = q_dims\n",
    "        self.dims = self.q_dims + self.p_dims[1:]\n",
    "        \n",
    "        self.lam = lam\n",
    "        self.lr = lr\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        self.construct_placeholders()\n",
    "\n",
    "    def construct_placeholders(self):        \n",
    "        self.input_ph = tf.placeholder(\n",
    "            dtype=tf.float32, shape=[None, self.dims[0]])\n",
    "        self.keep_prob_ph = tf.placeholder_with_default(1.0, shape=None)\n",
    "\n",
    "    def build_graph(self):\n",
    "\n",
    "        self.construct_weights()\n",
    "\n",
    "        saver, logits = self.forward_pass()\n",
    "        log_softmax_var = tf.nn.log_softmax(logits)\n",
    "\n",
    "        # per-user average negative log-likelihood\n",
    "        neg_ll = -tf.reduce_mean(tf.reduce_sum(\n",
    "            log_softmax_var * self.input_ph, axis=1))\n",
    "        # apply regularization to weights\n",
    "        reg = l2_regularizer(self.lam)\n",
    "        reg_var = apply_regularization(reg, self.weights)\n",
    "        # tensorflow l2 regularization multiply 0.5 to the l2 norm\n",
    "        # multiply 2 so that it is back in the same scale\n",
    "        loss = neg_ll + 2 * reg_var\n",
    "        \n",
    "        train_op = tf.train.AdamOptimizer(self.lr).minimize(loss)\n",
    "\n",
    "        # add summary statistics\n",
    "        tf.summary.scalar('negative_multi_ll', neg_ll)\n",
    "        tf.summary.scalar('loss', loss)\n",
    "        merged = tf.summary.merge_all()\n",
    "        return saver, logits, loss, train_op, merged\n",
    "\n",
    "    def forward_pass(self):\n",
    "        # construct forward graph        \n",
    "        h = tf.nn.l2_normalize(self.input_ph, 1)\n",
    "        h = tf.nn.dropout(h, self.keep_prob_ph)\n",
    "        \n",
    "        for i, (w, b) in enumerate(zip(self.weights, self.biases)):\n",
    "            h = tf.matmul(h, w) + b\n",
    "            \n",
    "            if i != len(self.weights) - 1:\n",
    "                h = tf.nn.tanh(h)\n",
    "        return tf.train.Saver(), h\n",
    "\n",
    "    def construct_weights(self):\n",
    "\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        # define weights\n",
    "        for i, (d_in, d_out) in enumerate(zip(self.dims[:-1], self.dims[1:])):\n",
    "            weight_key = \"weight_{}to{}\".format(i, i+1)\n",
    "            bias_key = \"bias_{}\".format(i+1)\n",
    "            \n",
    "            self.weights.append(tf.get_variable(\n",
    "                name=weight_key, shape=[d_in, d_out],\n",
    "                initializer=tf.contrib.layers.xavier_initializer(\n",
    "                    seed=self.random_seed)))\n",
    "            \n",
    "            self.biases.append(tf.get_variable(\n",
    "                name=bias_key, shape=[d_out],\n",
    "                initializer=tf.truncated_normal_initializer(\n",
    "                    stddev=0.001, seed=self.random_seed)))\n",
    "            \n",
    "            # add summary stats\n",
    "            tf.summary.histogram(weight_key, self.weights[-1])\n",
    "            tf.summary.histogram(bias_key, self.biases[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of Multi-VAE^{PR} (evidence lower-bound, or ELBO) for a single user $u$ is:\n",
    "$$\n",
    "\\mathcal{L}_u(\\theta, \\phi) = \\mathbb{E}_{q_\\phi(z_u | x_u)}[\\log p_\\theta(x_u | z_u)] - \\beta \\cdot KL(q_\\phi(z_u | x_u) \\| p(z_u))\n",
    "$$\n",
    "where $q_\\phi$ is the approximating variational distribution (inference model). $\\beta$ is the additional annealing parameter that we control. The objective of the entire dataset is the average over all the users. It can be trained almost the same as Multi-DAE, thanks to reparametrization trick. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiVAE(MultiDAE):\n",
    "\n",
    "    def construct_placeholders(self):\n",
    "        super(MultiVAE, self).construct_placeholders()\n",
    "\n",
    "        # placeholders with default values when scoring\n",
    "        self.is_training_ph = tf.placeholder_with_default(0., shape=None)\n",
    "        self.anneal_ph = tf.placeholder_with_default(1., shape=None)\n",
    "        \n",
    "    def build_graph(self):\n",
    "        self._construct_weights()\n",
    "\n",
    "        saver, logits, KL = self.forward_pass()\n",
    "        log_softmax_var = tf.nn.log_softmax(logits)\n",
    "\n",
    "        neg_ll = -tf.reduce_mean(tf.reduce_sum(\n",
    "            log_softmax_var * self.input_ph,\n",
    "            axis=-1))\n",
    "        # apply regularization to weights\n",
    "        reg = l2_regularizer(self.lam)\n",
    "        \n",
    "        reg_var = apply_regularization(reg, self.weights_q + self.weights_p)\n",
    "        # tensorflow l2 regularization multiply 0.5 to the l2 norm\n",
    "        # multiply 2 so that it is back in the same scale\n",
    "        neg_ELBO = neg_ll + self.anneal_ph * KL + 2 * reg_var\n",
    "        \n",
    "        train_op = tf.train.AdamOptimizer(self.lr).minimize(neg_ELBO)\n",
    "\n",
    "        # add summary statistics\n",
    "        tf.summary.scalar('negative_multi_ll', neg_ll)\n",
    "        tf.summary.scalar('KL', KL)\n",
    "        tf.summary.scalar('neg_ELBO_train', neg_ELBO)\n",
    "        merged = tf.summary.merge_all()\n",
    "\n",
    "        return saver, logits, neg_ELBO, train_op, merged\n",
    "    \n",
    "    def q_graph(self):\n",
    "        mu_q, std_q, KL = None, None, None\n",
    "        \n",
    "        h = tf.nn.l2_normalize(self.input_ph, 1)\n",
    "        h = tf.nn.dropout(h, self.keep_prob_ph)\n",
    "        \n",
    "        for i, (w, b) in enumerate(zip(self.weights_q, self.biases_q)):\n",
    "            h = tf.matmul(h, w) + b\n",
    "            \n",
    "            if i != len(self.weights_q) - 1:\n",
    "                h = tf.nn.tanh(h)\n",
    "            else:\n",
    "                mu_q = h[:, :self.q_dims[-1]]\n",
    "                logvar_q = h[:, self.q_dims[-1]:]\n",
    "\n",
    "                std_q = tf.exp(0.5 * logvar_q)\n",
    "                KL = tf.reduce_mean(tf.reduce_sum(\n",
    "                        0.5 * (-logvar_q + tf.exp(logvar_q) + mu_q**2 - 1), axis=1))\n",
    "        return mu_q, std_q, KL\n",
    "\n",
    "    def p_graph(self, z):\n",
    "        h = z\n",
    "        \n",
    "        for i, (w, b) in enumerate(zip(self.weights_p, self.biases_p)):\n",
    "            h = tf.matmul(h, w) + b\n",
    "            \n",
    "            if i != len(self.weights_p) - 1:\n",
    "                h = tf.nn.tanh(h)\n",
    "        return h\n",
    "\n",
    "    def forward_pass(self):\n",
    "        # q-network\n",
    "        mu_q, std_q, KL = self.q_graph()\n",
    "        epsilon = tf.random_normal(tf.shape(std_q))\n",
    "\n",
    "        sampled_z = mu_q + self.is_training_ph *\\\n",
    "            epsilon * std_q\n",
    "\n",
    "        # p-network\n",
    "        logits = self.p_graph(sampled_z)\n",
    "        \n",
    "        return tf.train.Saver(), logits, KL\n",
    "\n",
    "    def _construct_weights(self):\n",
    "        self.weights_q, self.biases_q = [], []\n",
    "        \n",
    "        for i, (d_in, d_out) in enumerate(zip(self.q_dims[:-1], self.q_dims[1:])):\n",
    "            if i == len(self.q_dims[:-1]) - 1:\n",
    "                # we need two sets of parameters for mean and variance,\n",
    "                # respectively\n",
    "                d_out *= 2\n",
    "            weight_key = \"weight_q_{}to{}\".format(i, i+1)\n",
    "            bias_key = \"bias_q_{}\".format(i+1)\n",
    "            \n",
    "            self.weights_q.append(tf.get_variable(\n",
    "                name=weight_key, shape=[d_in, d_out],\n",
    "                initializer=tf.contrib.layers.xavier_initializer(\n",
    "                    seed=self.random_seed)))\n",
    "            \n",
    "            self.biases_q.append(tf.get_variable(\n",
    "                name=bias_key, shape=[d_out],\n",
    "                initializer=tf.truncated_normal_initializer(\n",
    "                    stddev=0.001, seed=self.random_seed)))\n",
    "            \n",
    "            # add summary stats\n",
    "            tf.summary.histogram(weight_key, self.weights_q[-1])\n",
    "            tf.summary.histogram(bias_key, self.biases_q[-1])\n",
    "            \n",
    "        self.weights_p, self.biases_p = [], []\n",
    "\n",
    "        for i, (d_in, d_out) in enumerate(zip(self.p_dims[:-1], self.p_dims[1:])):\n",
    "            weight_key = \"weight_p_{}to{}\".format(i, i+1)\n",
    "            bias_key = \"bias_p_{}\".format(i+1)\n",
    "            self.weights_p.append(tf.get_variable(\n",
    "                name=weight_key, shape=[d_in, d_out],\n",
    "                initializer=tf.contrib.layers.xavier_initializer(\n",
    "                    seed=self.random_seed)))\n",
    "            \n",
    "            self.biases_p.append(tf.get_variable(\n",
    "                name=bias_key, shape=[d_out],\n",
    "                initializer=tf.truncated_normal_initializer(\n",
    "                    stddev=0.001, seed=self.random_seed)))\n",
    "            \n",
    "            # add summary stats\n",
    "            tf.summary.histogram(weight_key, self.weights_p[-1])\n",
    "            tf.summary.histogram(bias_key, self.biases_p[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/validation data, hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pre-processed training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sid = list()\n",
    "with open(os.path.join(pro_dir, 'unique_sid.txt'), 'r') as f:\n",
    "    for line in f:\n",
    "        unique_sid.append(line.strip())\n",
    "\n",
    "n_items = len(unique_sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data(csv_file):\n",
    "    tp = pd.read_csv(csv_file)\n",
    "    n_users = tp['uid'].max() + 1\n",
    "\n",
    "    rows, cols = tp['uid'], tp['sid']\n",
    "    data = sparse.csr_matrix((np.ones_like(rows),\n",
    "                             (rows, cols)), dtype='float64',\n",
    "                             shape=(n_users, n_items))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_train_data(os.path.join(pro_dir, 'train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tr_te_data(csv_file_tr, csv_file_te):\n",
    "    tp_tr = pd.read_csv(csv_file_tr)\n",
    "    tp_te = pd.read_csv(csv_file_te)\n",
    "\n",
    "    start_idx = min(tp_tr['uid'].min(), tp_te['uid'].min())\n",
    "    end_idx = max(tp_tr['uid'].max(), tp_te['uid'].max())\n",
    "\n",
    "    rows_tr, cols_tr = tp_tr['uid'] - start_idx, tp_tr['sid']\n",
    "    rows_te, cols_te = tp_te['uid'] - start_idx, tp_te['sid']\n",
    "\n",
    "    data_tr = sparse.csr_matrix((np.ones_like(rows_tr),\n",
    "                             (rows_tr, cols_tr)), dtype='float64', shape=(end_idx - start_idx + 1, n_items))\n",
    "    data_te = sparse.csr_matrix((np.ones_like(rows_te),\n",
    "                             (rows_te, cols_te)), dtype='float64', shape=(end_idx - start_idx + 1, n_items))\n",
    "    return data_tr, data_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vad_data_tr, vad_data_te = load_tr_te_data(os.path.join(pro_dir, 'validation_tr.csv'),\n",
    "                                           os.path.join(pro_dir, 'validation_te.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74375, 9950)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = train_data.shape[0]\n",
    "idxlist = range(N)\n",
    "\n",
    "# training batch size\n",
    "batch_size = 500\n",
    "batches_per_epoch = int(np.ceil(float(N) / batch_size))\n",
    "\n",
    "N_vad = vad_data_tr.shape[0]\n",
    "idxlist_vad = range(N_vad)\n",
    "\n",
    "# validation batch size (since the entire validation set might not fit into GPU memory)\n",
    "batch_size_vad = 2000\n",
    "\n",
    "# the total number of gradient updates for annealing\n",
    "total_anneal_steps = 15000\n",
    "# largest annealing parameter\n",
    "anneal_cap = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate function: Normalized discounted cumulative gain (NDCG@k) and Recall@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NDCG_binary_at_k_batch(X_pred, heldout_batch, k=100):\n",
    "    '''\n",
    "    normalized discounted cumulative gain@k for binary relevance\n",
    "    ASSUMPTIONS: all the 0's in heldout_data indicate 0 relevance\n",
    "    '''\n",
    "    batch_users = X_pred.shape[0]\n",
    "    idx_topk_part = bn.argpartition(-X_pred, k, axis=1)\n",
    "    topk_part = X_pred[np.arange(batch_users)[:, np.newaxis],\n",
    "                       idx_topk_part[:, :k]]\n",
    "    idx_part = np.argsort(-topk_part, axis=1)\n",
    "    # X_pred[np.arange(batch_users)[:, np.newaxis], idx_topk] is the sorted\n",
    "    # topk predicted score\n",
    "    idx_topk = idx_topk_part[np.arange(batch_users)[:, np.newaxis], idx_part]\n",
    "    # build the discount template\n",
    "    tp = 1. / np.log2(np.arange(2, k + 2))\n",
    "\n",
    "    DCG = (heldout_batch[np.arange(batch_users)[:, np.newaxis],\n",
    "                         idx_topk].toarray() * tp).sum(axis=1)\n",
    "    IDCG = np.array([(tp[:min(n, k)]).sum()\n",
    "                     for n in heldout_batch.getnnz(axis=1)])\n",
    "    return DCG / IDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Recall_at_k_batch(X_pred, heldout_batch, k=100):\n",
    "    batch_users = X_pred.shape[0]\n",
    "\n",
    "    idx = bn.argpartition(-X_pred, k, axis=1)\n",
    "    X_pred_binary = np.zeros_like(X_pred, dtype=bool)\n",
    "    X_pred_binary[np.arange(batch_users)[:, np.newaxis], idx[:, :k]] = True\n",
    "\n",
    "    X_true_binary = (heldout_batch > 0).toarray()\n",
    "    tmp = (np.logical_and(X_true_binary, X_pred_binary).sum(axis=1)).astype(\n",
    "        np.float32)\n",
    "    recall = tmp / np.minimum(k, X_true_binary.sum(axis=1))\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Multi-VAE^{PR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ML-20M dataset, we set both the generative function $f_\\theta(\\cdot)$ and the inference model $g_\\phi(\\cdot)$ to be 3-layer multilayer perceptron (MLP) with symmetrical architecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generative function is a [200 -> 600 -> n_items] MLP, which means the inference function is a [n_items -> 600 -> 200] MLP. Thus the overall architecture for the Multi-VAE^{PR} is [n_items -> 600 -> 200 -> 600 -> n_items]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_dims = [200, 600, n_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\atazarv\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-36-881e9e51e708>:41: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "WARNING:tensorflow:From C:\\Users\\atazarv\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "vae = MultiVAE(p_dims, lam=0.0, random_seed=98765)\n",
    "\n",
    "saver, logits_var, loss_var, train_op_var, merged_var = vae.build_graph()\n",
    "\n",
    "ndcg_var = tf.Variable(0.0)\n",
    "ndcg_dist_var = tf.placeholder(dtype=tf.float64, shape=None)\n",
    "ndcg_summary = tf.summary.scalar('ndcg_at_k_validation', ndcg_var)\n",
    "ndcg_dist_summary = tf.summary.histogram('ndcg_at_k_hist_validation', ndcg_dist_var)\n",
    "merged_valid = tf.summary.merge([ndcg_summary, ndcg_dist_summary])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up logging and checkpoint directory\n",
    "\n",
    "- Change all the logging directory and checkpoint directory to somewhere of your choice\n",
    "- Monitor training progress using tensorflow by: `tensorboard --logdir=$log_dir`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_str = \"I-%s-I\" % ('-'.join([str(d) for d in vae.dims[1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log directory: /volmount/log/ml-20m/VAE_anneal15.0K_cap2.0E-01/I-600-200-600-I\n"
     ]
    }
   ],
   "source": [
    "log_dir = '/volmount/log/ml-20m/VAE_anneal{}K_cap{:1.1E}/{}'.format(\n",
    "#log_dir = 'D:/volmount/log/ml-20m/VAE_anneal{}K_cap{:1.1E}/{}'.format(\n",
    "    total_anneal_steps/1000, anneal_cap, arch_str)\n",
    "\n",
    "if os.path.exists(log_dir):\n",
    "    shutil.rmtree(log_dir)\n",
    "\n",
    "print(\"log directory: %s\" % log_dir)\n",
    "summary_writer = tf.summary.FileWriter(log_dir, graph=tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chkpt directory: /volmount/chkpt/ml-20m/VAE_anneal15.0K_cap2.0E-01/I-600-200-600-I\n"
     ]
    }
   ],
   "source": [
    "chkpt_dir = '/volmount/chkpt/ml-20m/VAE_anneal{}K_cap{:1.1E}/{}'.format(\n",
    "    total_anneal_steps/1000, anneal_cap, arch_str)\n",
    "\n",
    "if not os.path.isdir(chkpt_dir):\n",
    "    os.makedirs(chkpt_dir) \n",
    "    \n",
    "print(\"chkpt directory: %s\" % chkpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxlist = list(idxlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcgs_vad = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    best_ndcg = -np.inf\n",
    "\n",
    "    update_count = 0.0\n",
    "    start=datetime.now()\n",
    "    for epoch in range(n_epochs):\n",
    "        np.random.shuffle(idxlist)\n",
    "        # train for one epoch\n",
    "        for bnum, st_idx in enumerate(range(0, N, batch_size)):\n",
    "            end_idx = min(st_idx + batch_size, N)\n",
    "            X = train_data[idxlist[st_idx:end_idx]]\n",
    "            \n",
    "            if sparse.isspmatrix(X):\n",
    "                X = X.toarray()\n",
    "            X = X.astype('float32')           \n",
    "            \n",
    "            if total_anneal_steps > 0:\n",
    "                anneal = min(anneal_cap, 1. * update_count / total_anneal_steps)\n",
    "            else:\n",
    "                anneal = anneal_cap\n",
    "            \n",
    "            feed_dict = {vae.input_ph: X, \n",
    "                         vae.keep_prob_ph: 0.5, \n",
    "                         vae.anneal_ph: anneal,\n",
    "                         vae.is_training_ph: 1}        \n",
    "            sess.run(train_op_var, feed_dict=feed_dict)\n",
    "\n",
    "            if bnum % 100 == 0:\n",
    "                summary_train = sess.run(merged_var, feed_dict=feed_dict)\n",
    "                summary_writer.add_summary(summary_train, \n",
    "                                           global_step=epoch * batches_per_epoch + bnum) \n",
    "            \n",
    "            update_count += 1\n",
    "        \n",
    "        # compute validation NDCG\n",
    "        ndcg_dist = []\n",
    "        for bnum, st_idx in enumerate(range(0, N_vad, batch_size_vad)):\n",
    "            end_idx = min(st_idx + batch_size_vad, N_vad)\n",
    "            X = vad_data_tr[idxlist_vad[st_idx:end_idx]]\n",
    "\n",
    "            if sparse.isspmatrix(X):\n",
    "                X = X.toarray()\n",
    "            X = X.astype('float32')\n",
    "        \n",
    "            pred_val = sess.run(logits_var, feed_dict={vae.input_ph: X} )\n",
    "            # exclude examples from training and validation (if any)\n",
    "            pred_val[X.nonzero()] = -np.inf\n",
    "            ndcg_dist.append(NDCG_binary_at_k_batch(pred_val, vad_data_te[idxlist_vad[st_idx:end_idx]]))\n",
    "        \n",
    "        ndcg_dist = np.concatenate(ndcg_dist)\n",
    "        ndcg_ = ndcg_dist.mean()\n",
    "        ndcgs_vad.append(ndcg_)\n",
    "        merged_valid_val = sess.run(merged_valid, feed_dict={ndcg_var: ndcg_, ndcg_dist_var: ndcg_dist})\n",
    "        summary_writer.add_summary(merged_valid_val, epoch)\n",
    "\n",
    "        # update the best model (if necessary)\n",
    "        if ndcg_ > best_ndcg:\n",
    "            saver.save(sess, '{}/model'.format(chkpt_dir))\n",
    "            best_ndcg = ndcg_\n",
    "        print(\"run time for the %d th epoch: \" %epoch, datetime.now()-start)\n",
    "        start = datetime.now()\n",
    "        with open('ndcgs_vad_vae_beta02.csv', 'wb') as fp:\n",
    "            pickle.dump(ndcgs_vad, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 3))\n",
    "plt.plot(ndcgs_vad)\n",
    "plt.ylabel(\"Validation NDCG@100\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(asghar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write\n",
    "with open('ndcgs_vad_vae_beta02.csv', 'wb') as fp:\n",
    "    pickle.dump(ndcgs_vad, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read\n",
    "with open ('ndcgs_vad_vae_betta02.csv', 'rb') as fp:\n",
    "    itemlist = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemlist[13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the test data and compute test metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_tr, test_data_te = load_tr_te_data(\n",
    "    os.path.join(pro_dir, 'test_tr.csv'),\n",
    "    os.path.join(pro_dir, 'test_te.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_test = test_data_tr.shape[0]\n",
    "idxlist_test = range(N_test)\n",
    "\n",
    "batch_size_test = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Scale of 0 disables regularizer.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "vae = MultiVAE(p_dims, lam=0.0)\n",
    "saver, logits_var, _, _, _ = vae.build_graph()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the best performing model on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chkpt directory: /volmount/chkpt/ml-20m/VAE_anneal15.0K_cap2.0E-01/I-600-200-600-I\n"
     ]
    }
   ],
   "source": [
    "chkpt_dir = '/volmount/chkpt/ml-20m/VAE_anneal{}K_cap{:1.1E}/{}'.format(\n",
    "    total_anneal_steps/1000, anneal_cap, arch_str)\n",
    "print(\"chkpt directory: %s\" % chkpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\atazarv\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from /volmount/chkpt/ml-20m/VAE_anneal15.0K_cap2.0E-01/I-600-200-600-I/model\n"
     ]
    }
   ],
   "source": [
    "n100_list, r20_list, r50_list = [], [], []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, '{}/model'.format(chkpt_dir))\n",
    "\n",
    "    for bnum, st_idx in enumerate(range(0, N_test, batch_size_test)):\n",
    "        end_idx = min(st_idx + batch_size_test, N_test)\n",
    "        X = test_data_tr[idxlist_test[st_idx:end_idx]]\n",
    "\n",
    "        if sparse.isspmatrix(X):\n",
    "            X = X.toarray()\n",
    "        X = X.astype('float32')\n",
    "\n",
    "        pred_val = sess.run(logits_var, feed_dict={vae.input_ph: X})\n",
    "        # exclude examples from training and validation (if any)\n",
    "        pred_val[X.nonzero()] = -np.inf\n",
    "        n100_list.append(NDCG_binary_at_k_batch(pred_val, test_data_te[idxlist_test[st_idx:end_idx]], k=100))\n",
    "        r20_list.append(Recall_at_k_batch(pred_val, test_data_te[idxlist_test[st_idx:end_idx]], k=20))\n",
    "        r50_list.append(Recall_at_k_batch(pred_val, test_data_te[idxlist_test[st_idx:end_idx]], k=50))\n",
    "    \n",
    "n100_list = np.concatenate(n100_list)\n",
    "r20_list = np.concatenate(r20_list)\n",
    "r50_list = np.concatenate(r50_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test NDCG@100=0.53100 (0.00173)\n",
      "Test Recall@20=0.39663 (0.00181)\n",
      "Test Recall@50=0.57547 (0.00195)\n"
     ]
    }
   ],
   "source": [
    "print(\"Test NDCG@100=%.5f (%.5f)\" % (np.mean(n100_list), np.std(n100_list) / np.sqrt(len(n100_list))))\n",
    "print(\"Test Recall@20=%.5f (%.5f)\" % (np.mean(r20_list), np.std(r20_list) / np.sqrt(len(r20_list))))\n",
    "print(\"Test Recall@50=%.5f (%.5f)\" % (np.mean(r50_list), np.std(r50_list) / np.sqrt(len(r50_list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(asghar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Multi-DAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generative function is a [200 -> n_items] MLP, thus the overall architecture for the Multi-DAE is [n_items -> 200 -> n_items]. We find this architecture achieves better validation NDCG@100 than the [n_items -> 600 -> 200 -> 600 -> n_items] architecture as used in Multi-VAE^{PR}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_dims = [200, n_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "dae = MultiDAE(p_dims, lam=0.01 / batch_size, random_seed=98765)\n",
    "\n",
    "saver, logits_var, loss_var, train_op_var, merged_var = dae.build_graph()\n",
    "\n",
    "ndcg_var = tf.Variable(0.0)\n",
    "ndcg_dist_var = tf.placeholder(dtype=tf.float64, shape=None)\n",
    "ndcg_summary = tf.summary.scalar('ndcg_at_k_validation', ndcg_var)\n",
    "ndcg_dist_summary = tf.summary.histogram('ndcg_at_k_hist_validation', ndcg_dist_var)\n",
    "merged_valid = tf.summary.merge([ndcg_summary, ndcg_dist_summary])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up logging and checkpoint directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_str = \"I-%s-I\" % ('-'.join([str(d) for d in dae.dims[1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = '/volmount/log/ml-20m/DAE/{}'.format(arch_str)\n",
    "\n",
    "if os.path.exists(log_dir):\n",
    "    shutil.rmtree(log_dir)\n",
    "\n",
    "print(\"log directory: %s\" % log_dir)\n",
    "summary_writer = tf.summary.FileWriter(log_dir, graph=tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chkpt_dir = '/volmount/chkpt/ml-20m/DAE/{}'.format(arch_str)\n",
    "\n",
    "if not os.path.isdir(chkpt_dir):\n",
    "    os.makedirs(chkpt_dir) \n",
    "    \n",
    "print(\"chkpt directory: %s\" % chkpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcgs_vad = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    best_ndcg = -np.inf\n",
    "    start = datetime.now()\n",
    "    for epoch in range(n_epochs):\n",
    "        np.random.shuffle(idxlist)\n",
    "        # train for one epoch\n",
    "        for bnum, st_idx in enumerate(range(0, N, batch_size)):\n",
    "            end_idx = min(st_idx + batch_size, N)\n",
    "            X = train_data[idxlist[st_idx:end_idx]]\n",
    "            \n",
    "            if sparse.isspmatrix(X):\n",
    "                X = X.toarray()\n",
    "            X = X.astype('float32')           \n",
    "            \n",
    "            feed_dict = {dae.input_ph: X, \n",
    "                         dae.keep_prob_ph: 0.5}        \n",
    "            sess.run(train_op_var, feed_dict=feed_dict)\n",
    "\n",
    "            if bnum % 100 == 0:\n",
    "                summary_train = sess.run(merged_var, feed_dict=feed_dict)\n",
    "                summary_writer.add_summary(summary_train, global_step=epoch * batches_per_epoch + bnum) \n",
    "                    \n",
    "        # compute validation NDCG\n",
    "        ndcg_dist = []\n",
    "        for bnum, st_idx in enumerate(range(0, N_vad, batch_size_vad)):\n",
    "            end_idx = min(st_idx + batch_size_vad, N_vad)\n",
    "            X = vad_data_tr[idxlist_vad[st_idx:end_idx]]\n",
    "\n",
    "            if sparse.isspmatrix(X):\n",
    "                X = X.toarray()\n",
    "            X = X.astype('float32')\n",
    "        \n",
    "            pred_val = sess.run(logits_var, feed_dict={dae.input_ph: X} )\n",
    "            # exclude examples from training and validation (if any)\n",
    "            pred_val[X.nonzero()] = -np.inf\n",
    "            ndcg_dist.append(NDCG_binary_at_k_batch(pred_val, vad_data_te[idxlist_vad[st_idx:end_idx]]))\n",
    "        \n",
    "        ndcg_dist = np.concatenate(ndcg_dist)\n",
    "        ndcg_ = ndcg_dist.mean()\n",
    "        ndcgs_vad.append(ndcg_)\n",
    "        merged_valid_val = sess.run(merged_valid, feed_dict={ndcg_var: ndcg_, ndcg_dist_var: ndcg_dist})\n",
    "        summary_writer.add_summary(merged_valid_val, epoch)\n",
    "\n",
    "        # update the best model (if necessary)\n",
    "        if ndcg_ > best_ndcg:\n",
    "            saver.save(sess, '{}/model'.format(chkpt_dir))\n",
    "            best_ndcg = ndcg_\n",
    "        print(\"run time for the %d th epoch: \" %epoch, datetime.now()-start)\n",
    "        start = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 3))\n",
    "plt.plot(ndcgs_vad)\n",
    "plt.ylabel(\"Validation NDCG@100\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write\n",
    "with open('ndcgs_vad_dae.csv', 'wb') as fp:\n",
    "    pickle.dump(ndcgs_vad, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read\n",
    "with open ('ndcgs_vad_dae.csv', 'rb') as fp:\n",
    "    itemlist = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute test metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "dae = MultiDAE(p_dims, lam=0.01 / batch_size)\n",
    "saver, logits_var, _, _, _ = dae.build_graph()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the best performing model on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chkpt_dir = '/volmount/chkpt/ml-20m/DAE/{}'.format(arch_str)\n",
    "print(\"chkpt directory: %s\" % chkpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n100_list, r20_list, r50_list = [], [], []\n",
    "\n",
    "with tf.Session() as sess:    \n",
    "    saver.restore(sess, '{}/model'.format(chkpt_dir))\n",
    "    \n",
    "    for bnum, st_idx in enumerate(range(0, N_test, batch_size_test)):\n",
    "        end_idx = min(st_idx + batch_size_test, N_test)\n",
    "        X = test_data_tr[idxlist_test[st_idx:end_idx]]\n",
    "\n",
    "        if sparse.isspmatrix(X):\n",
    "            X = X.toarray()\n",
    "        X = X.astype('float32')\n",
    "\n",
    "        pred_val = sess.run(logits_var, feed_dict={dae.input_ph: X})\n",
    "        # exclude examples from training and validation (if any)\n",
    "        pred_val[X.nonzero()] = -np.inf\n",
    "        n100_list.append(NDCG_binary_at_k_batch(pred_val, test_data_te[idxlist_test[st_idx:end_idx]], k=100))\n",
    "        r20_list.append(Recall_at_k_batch(pred_val, test_data_te[idxlist_test[st_idx:end_idx]], k=20))\n",
    "        r50_list.append(Recall_at_k_batch(pred_val, test_data_te[idxlist_test[st_idx:end_idx]], k=50))\n",
    "\n",
    "n100_list = np.concatenate(n100_list)\n",
    "r20_list = np.concatenate(r20_list)\n",
    "r50_list = np.concatenate(r50_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test NDCG@100=%.5f (%.5f)\" % (np.mean(n100_list), np.std(n100_list) / np.sqrt(len(n100_list))))\n",
    "print(\"Test Recall@20=%.5f (%.5f)\" % (np.mean(r20_list), np.std(r20_list) / np.sqrt(len(r20_list))))\n",
    "print(\"Test Recall@50=%.5f (%.5f)\" % (np.mean(r50_list), np.std(r50_list) / np.sqrt(len(r50_list))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
